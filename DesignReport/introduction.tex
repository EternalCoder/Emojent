\chapter{Introduction}
Everyday there are thousands of people who struggle with social issues and related anxieties. One major aspect of this issue stems from many people's inability to easily recognize another person's emotions, especially the fleeting and nonverbal cues that communicate an individual's emotional state. Struggling with reading other people's facial expressions can lead to severe social anxiety and problems connecting with others. While this ability comes naturally to a majority of people, individuals affected by autism spectrum disorder generally have a harder time identifying the same facial expressions. These people can end up feeling ostracized and alone because of their inability to detect emotions and nonverbal cues. As technology has advanced, emotional recognition has become a topic of significant research and study. Providing those who have these issues with a simple and cost-efficient way of clearly detecting emotions would have an immense impact on their lives by helping them communicate and connect with the rest of society.
\par
	While some emotional recognition algorithms do exist, there are very few complete services that can actually be used by people with autism spectrum disorder. Many of the current “solutions” are APIs that offer facial and emotional recognition capabilities but have not been implemented to operate on live video streams, which has limited their real world use. For example, Microsoft has created an API that can detect people’s emotions, but none of the apps implementing it use Augmented Reality (AR) to make seeing emotions easy. Yet another research group, Affectiva, has an emotion recognition SDK that can track emotions in real-time video. A related app exists for Google Glass, but the system commands a high price and Google no longer produces it for the general public. Currently, a cost-effective, accessible solution does not exist. Forest Handford, an Affectiva DevOps Lead, has even blogged about the need for such an application. Unfortunately, no such app has been created, so the problem remains.
\par
	We propose a mobile application that uses computer vision libraries and a pre-existing emotion recognition API to process live video and display a filter that gives the user an estimation of another person's emotional state, based on their facial expression. Our application will improve upon other solutions because its all-in-one nature will only require a device with a camera and system hardware capable of supporting common computer vision libraries. For this reason, our solution will be much more accessible and impactful than other existing solutions.