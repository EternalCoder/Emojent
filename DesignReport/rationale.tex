\chapter{Design Rationale}
Emojent is an android application that targets everyone diagnosed with Autism Spectrum Disorders(ASD). These people’s ages can range from young to old. As a result, we have a wide range of people to satisfy. All of them will be using our app to help with their social interactions. Younger people can use the app to make friends more easily, adults can use the app to make it easier to find jobs, and the elderly can use the app to communicate with their children or grandchildren. We expect people with ASD will use Emojent on a daily basis so we will make the app simple and easy to use, but keep it aesthetically pleasing. When the app is opened, the first screen will show two icons next to each other in the middle of the screen. The first icon will be to view emotions through a live video feed and the second icon will be to view emotions through a live audio feed.

After clicking on the video icon, you will be taken to a page with a video camera. There will be a start button in the bottom-middle of the screen and a smaller audio button on the bottom-right. Clicking the start button will start the facial expression analyzer. After every few frames, a photo is taken and sent to Microsoft’s emotion api. The emotions corresponding to the photo are updated on the screen. The primary emotion is shown using an emoji. Augmented reality will be used so the video looks realistic but with emotion values and emojis next to it. The audio button will take you to the audio page. We decided to only have 2 buttons to make the app simple. Instead of using the entire real-time video, we decided to use photos to make it easier on the phone. Augmented reality will make the app look more realistic and fun to use. The emojis are mostly for younger children since they’d have an easier time understanding them. 

Clicking on the audio icon will take you to a page that uses a mic. Similar to the video page, there will be a start button and a smaller video button. Since there is no video, there will be a single color background. Having both pages look similar will make the app more user-friendly and consistent. Clicking on the start button starts the mic and the audio is recorded. Beyond Verbal’s api is then used analyze the audio. The emotions are then shown on the screen in a list format. The primary emotion is shown using an emoji. Similarly, clicking on the video button will take you to the video page.

The technologies we will use to make our mobile app are Java, Microsoft’s emotion api, Beyond Verbal’s emotion analytics api, an Android phone, Android Studio, Github, and Slack. We will use Java because it is the main programming language used in Android app development. Microsoft’s emotion api and Beyond Verbal’s emotion analytics api will be used to calculate the emotion values from facial expressions and vocal tones respectively. We chose those two apis over the others because Microsoft is a well known company with a lot of resources and Beyond Verbal has been researching for 21 years and has close to 2 million voice samples. An Android phone is required for us to test the app. Testing on a real phone will work much better than testing on an emulator. We also decided to use Android Studio because it is the official IDE for android development. The last two technologies we used were to make working together with each other easy. Github is our backup and code management system where everyone can store the code in one place. We chose Github because it is free and easy to use. To make communicating between group members easier, we used Slack. Slack also integrates github so we can easily keep track of changes to the repository.